\documentclass[letterpaper,11pt,twocolumn]{article}
\include{template}
\usepackage{multicol}
\usepackage{fullpage}
\usepackage{url}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{framed}
\usepackage{enumerate}
\pagenumbering{arabic}
\setlength{\columnsep}{0.25in}
\usepackage{paralist}
\let\itemize\compactitem

\def\bfw{\mathbf w}
\def\bfW{\mathbf W}
\def\bfD{\mathbf D}
\def\bfY{\mathbf Y}
\def\bfX{\mathbf X}
\def\bfU{\mathbf U}
\def\bfV{\mathbf V}
\def\bfS{\mathbf \Sigma}
\def\bfx{\mathbf x}
\def\R{\mathbb R}
\def\F{\mathrm F}

\title{\bf{Twitter hashtag implication using various machine learning methods}}
\author{
  Chang Wang\\
  Department of Computer Sciences\\
  \texttt{wych92@cs.wisc.edu}
  \and
  Lichao Yin\\
  Department of Computer Sciences\\
  \texttt{lyin28@wisc.edu}
  \and
  Chaowen Yu\\
  Department of Computer Sciences\\
  \texttt{ycw@cs.wisc.edu}
  \and
  Biao Zhang\\
  Department of Computer Sciences\\
  \texttt{bzhang263@wisc.edu}
}
\date{May 9, 2015}
\begin{document}
\twocolumn[
\maketitle
]
\begin{abstract}
\textbf{keywords}: supervised learning, text classification
\end{abstract}


\section{Introduction}
\label{sec:intro}

In this ever-changing world, information is produced and consumed ever faster than we could imagine. Every second, Twitter users post 6000 tweets, which corresponds to 350,000 tweets per minute, and 500 million tweets per day. With such amount of information generated, it can be formidable to search for a specific piece of information, if not completely impossible. That is why hashtag (\#) comes into being. Formally, hashtag (\#) is a character string that indicates the topic or category of the tweet. A tweet can have more than one hashtag and thus related to multiple topics. Hashtag greatly facilitates information filtering and rearrangements as searching for a tweet can be simplified to searching for a hashtag or even multiple hashtags.

However, not every user is used to hashtagging a tweet: they may forget to hashtag a tweet or they may use rare hashtags that could barely be keyword in searching or filtering. On the other hand, manually fixing a million tweets seems impractical even if tagless or bad-tagged tweets take up only 0.2\% of all tweets posted every day.

Our project aims to solve this problem using various machine learning methods. Given the text of a tweet, our task is to imply an appropriate hashtag that indicates the category or topic of this tweet. Formally, we define the hashtag implication problem as a supervised learning classification model as follows.

\begin{definition}[Hashtag Implication Problem]
Given a $S$ set of candidate hashtags as class labels, and a set $T$ of tagged tweets as training set. Learn a model $H$ that predicts one hashtag from $S$ for an untagged tweet with high accuracy.
\end{definition}

We do not specify deliberately which machine learning model we will be using as long as it is supervised learning. This is because it is not clear at this moment which model has the best performance. We feel it is wise to explore various machine learning methods, compare their performances and find out which machine learning method is the most accurate on hashtag implication problem.

The rest of our report is structures as follows: Section \ref{sec:data} 

\section{Data Preparation}
\label{sec:data}
\subsection{Data Collection}
\subsection{Data Preprocessing}
After above steps, we get 5000 tweets with corresponding hashtags like this:
\begin{framed}
The House of Black and White - The Wars to Come - \#gameofthrones \#got \#got5 \#e2s5  \#hbo $\backslash$u2026 https://t.co/6zrQXilrZe
\end{framed}
It is obvious that \emph{http://}, \emph{$\backslash$u2026} and \emph{\#gameofthrones} do little contribution to classification. So we find them out through 5000 tweets and remove them. For convinience, we then remove all non-alphanumeric characters and turn all letters to lowercase. Thus the example tweet turns to be:
\begin{framed}
the house of black and white the wars to come
\end{framed}
Because computer usually fails to capture semantic information from human languages, it is common to represent a text as a vector corresponding to the terms(usually terms are words) that appear in the text. For our problem, we will use bag-of-words model. Here we define the following terms as denoted as \cite{blei2003latent}:
\begin{itemize}
\item A \emph{term} is the basic unit of discrete data denoted by $w$, which is usually a word appearing in a tweet.
\item A \emph{tweet} contains $N$ terms, denoted by $\bfw = (w_1, w_2, \dots, w_N)$.
\item A \emph{corpus} is a collection of $M$ tweets($M=5000$). The tweets texts are denoted by $\bfD = \{\bfw_1, \bfw_2, \dots, \bfw_M\}$, and their corresponding tag labels are denoted by $\bfY = \{y_1, y_2, \dots, y_M\}$ where $y_i \in \{1, 2, 3, 4, 5\}, i = 1, 2, \dots, M$. For our problem, 1 stands for the hashtag "", 2 stands for the hashtag "", 3 stands for the hashtag "", 4 stands for the hashtag "", and 5 stands for the hashtag "".
\item A \emph{dictionary} is a collection of all $V$ unique terms appearing in the corpus denoted by $\bfW = \{w_1^*, w_2^*, \dots, w_V^*\}$.
\end{itemize}

Let $\bfX$ be a term-tweet matrix. This matrix has $M$ rows(one row for each tweets) and $V$ columns(one column for each unique term in dictionary). The element $x_{ij}$ in $\bfX$ indicates whether the $j$-th term appears in the $i$-th tweet. If it does appear, $x_{ij} = 1$; otherwise $x_{ij} = 0$.

In general, because most tweets will just use a small part of terms in the dictionary, a lot of $x_{ij}$ will be zero. So the matrix $\bfX$ is sparse. However, a sparse $\bfX$ wastes too much space and processes much slower in high dimension. As we know, there are some high-frequency terms appearing in all documents, such as function words(e.g., \emph{a}, \emph{or}, \emph{on}) and pronouns(e.g., \emph{which}, \emph{it}, \emph{those}). These terms have little contribution to representing different documents because of relatively low information content, so we shall remove them from our dictionary. \cite{salton1971smart} developed a SMART system with a popular list of more than 500 common terms, which called \emph{stop-word list} and can be used for our model.

After removing \emph{stop-words}, our final $\bfX$ has 5000 rows for tweets and 9791 columns for unique terms, and $\bfY$ has 5000 rows for each tweet's tag label.
\subsection{Cross Validation}
We will use stratified sampling for cross validation.

Since we have 1000 tweets for every 5 hashtags. We equally split each 1000 hashtag tweets into 5 parts. And each time we select and combine 4 parts from every hashtags for training and the rest part for testing.

Therefore, we have 5 datasets in all. Each dataset has training matricies: $\bfX_{training} \in \R^{4000 \times 9791}$ and $\bfY_{training} \in \{1, 2, 3, 4, 5\}^{4000}$, and testing matrices $\bfX_{testing} \in \R^{1000 \times 9791}$ and $\bfY_{testing} \in \{1, 2, 3, 4, 5\}^{1000}$.

\section{Implementation}
\label{sec:impl}
\subsection{Decision Tree}
Decision tree is a simple but powerful machine-learning model. The advantages of decision trees are: 1. Easy to interpret and explain. 2. Non-parametric, so you don't have to worry about outliers or whether the data is linearly separable. The disadvantage is also obvious that they easily overfit.

To learn a multi-class decision tree, we use the implementation from scikit-learn 1.6.1. scikit-learn uses an optimised version of the CART algorithm.

We use scikit-learn default configuration to finish our task. The default configurations are as follows:

\begin{itemize}
\item \emph{criterion}: Gini impurity function is used to measure the quality of a split.
\item \emph{splitter strategy}: choose the “best” at when splitting.
\item \emph{number of features to consider when looking for the best split}: consider max number of features at each split since all features are of integer type.
\item \emph{maximum depth of the tree}: nodes are expanded until all leaves are pure or until all leaves contain less than a threshold.
\item \emph{minimum number of samples for splitting}: The minimum number of samples required to split an internal node. Set to 2.
\item \emph{minimum number of samples for a leaf node }: The minimum number of samples required to be at a leaf node. Set to 1.

\end{itemize}
\subsection{Random Forest}
Random Forest is an ensemble method that produces a highly accurate classifier and learns fast. It runs efficiently on large data bases and handles large amount of input variables without variable deletion or selection.

We download an Matlab implementation code from \emph{https://code.google.com/p/randomforest-matlab/}. For our problem we set the number of trees is 500 and the maximum trial times is 98.

\subsection{Neural Network}
Neural networks are algorithms that can be used to perform nonlinear statistical modeling and provide a new alternative to logistic regression, the most commonly used method for developing predictive models. Neural network requires less formal statistical training and implicitly detects complex nonlinear relationships between dependent and independent variables.

For our classification problem, we use neural network toolbox in matlab. We just use one hidden layer with size nou
\subsection{Na{\"i}ve Bayes}
The na{\"i}ve Bayes algorithm is one of the two classic na{\"i}ve Bayes variants used in text classification where the data are typically represented as word vector counts. The biggest advantages of na{\"i}ve Bayes is that it is super simple. Furthermore, sometimes even if the na{\"i}ve Bayes assumption doesn't hold, a na{\"i}ve Bayes classifier still often performs surprisingly well in practice.

The implementation of na{\"i}ve Bayes we used is the multinomial na{\"i}ve Bayes MultinomialNB from scikit-learn 1.6.1.

The configuration we used in our project is the default one, which are as follows:
\begin{itemize}
\item \emph{additive (Laplace/Lidstone) smoothing parameter }: 1.0
\item \emph{whether to learn class prior probabilities or not}: True
\item \emph{prior probabilities of the classes}: None
\end{itemize}

\subsection{Support Vector Machine}
Advantages of SVMs: High accuracy, nice theoretical guarantees regarding overfitting, and with an appropriate kernel they can work well even if you're data isn't linearly separable in the base feature space. Especially popular in text classification problems where very high-dimensional spaces are the norm. Memory-intensive and kind of annoying to run and tune, though, so I think random forests are starting to steal the crown.


\subsection{Logistic Regression Model}
Advantages of Logistic Regression: Lots of ways to regularize your model, and you don't have to worry as much about your features being correlated, like you do in Naive Bayes. You also have a nice probabilistic interpretation, unlike decision trees or SVMs, and you can easily update your model to take in new data (using an online gradient descent method), again unlike decision trees or SVMs. Use it if you want a probabilistic framework (e.g., to easily adjust classification thresholds, to say when you're unsure, or to get confidence intervals) or if you expect to receive more training data in the future that you want to be able to quickly incorporate into your model.

\subsection{$k$ Nearest Neighbors}


\section{Results}
\label{sec:eval}
% \begin{figure}[!htb]
% \centering
% \epsfig{file=evaluation.eps, width=3.4in}
% \caption{Boot-up time for various configurations}
% \label{figure:eval}
% \end{figure}

% \begin{figure*}[!htb]
% \centering
% \epsfig{file=latencies.eps, width=6.8in}
% \caption{Latencies for various IPC methods on MicroswiftOS and archlinux}
% \label{figure:latency}
% \end{figure*}

% \begin{figure*}[!htb]
% \centering
% \epsfig{file=throughput.eps, width=6.8in}
% \caption{Throughputs for various IPC methods on MicroswiftOS and archlinux}
% \label{figure:throughput}
% \end{figure*}

\section{Related Works}
\label{sec:related}
\section{Future Work}
\label{sec:future}

% \bibliographystyle{abbrv}
\bibliographystyle{plain}
\bibliography{references}

\end{document}

% \begin{table}[!htb]
% \centering
% \begin{tabular}{|c||c|c|c|}
% \hline
% \multirow{3}{*}{\textbf{Benchmark}} & \textbf{Additional} & \textbf{Total} &\multirow{3}{*}{\textbf{Percentage}}\\ 
% &\textbf{Memory} &\textbf{Memory}&\\
% &\textbf{Accesses} &\textbf{Accesses} &\\ \hline \hline
% aster & 175481 & 2404970 & 0.0730\\ \hline 
% bzip2 & 1067380 & 9188892 & 0.1162\\ \hline 
% lbm   & 130990 & 5003775 & 0.0262\\ \hline 
% libquantum   & 54796 & 3092800 & 0.0177\\ \hline 
% milc  & 136746 & 5575859 & 0.0245\\ \hline 
% omnetpp  & 107165 & 287076 & 0.3733\\ \hline 
% \textbf{Average} & \textbf{-} &\textbf{- } &\textbf{0.1051}\\ \hline
% \end{tabular}
% \caption{Additional memory accesses for storing reuse data in a 1MB L2 reuse cache during a simulation period of 300M instructions}
% \label{table:additionalaccess}
% \end{table}

% \begin{figure}[!htb]
% \centering
% \epsfig{file=ReuseStruct.eps, height =2in, width =2in}
% \caption{Structure of our implementation for the reuse cache}
% \label{reuse:struct}
% \end{figure}

% \begin{table}[!htb]
% \centering
% \begin{tabular}{|c||c|c|c|}
% \hline
% \multirow{3}{*}{\textbf{benchmark}} & \textbf{memory} & \textbf{total} & \multirow{3}{*}{\textbf{percentage}}\\
% &\textbf{write} &\textbf{memory} &\\
% & \textbf{miss times} & \textbf{access times} & \\ \hline\hline
% bzip2 & 778893 & 16791412 & 0.0464\\ \hline 
% aster & 83555 & 4487131 & 0.0186\\ \hline 
% libquantum & 35243 & 6091860 & 0.0058\\ \hline 
% lbm & 72942 & 9862259 & 0.0074\\ \hline 
% milc & 69407 & 10820712 & 0.0064\\ \hline 
% omnetpp & 83508 & 416317 & 0.2006 \\ \hline

% \textbf{Average} &\textbf{-} &\textbf{-} &\textbf{0.0475}\\

% \hline
% \end{tabular}
% \caption{Memory write miss times in a 1MB reuse cache during a simulation period of 300M instructions}
% \label{table:writemiss}
% \end{table}



% \begin{table}[!htb]
% \centering
% \begin{tabular}{|l||r|}
% \hline
% L1 data cache size & 64KB\\
% \hline
% L1 data cache associativity & 4-way\\
% \hline
% L1 instruction cache size & 32KB\\
% \hline
% L1 instruction cache associativity & 4-way\\
% \hline
% L2 cache size & 1MB\\
% \hline
% L2 cache associativity & 8-way\\
% \hline
% Cache line size & 64B\\
% \hline
% Single memory size & 512 MB\\
% \hline
% \end{tabular}
% \caption{System configurations for baseline performance}
% \label{table:baselineconfig}
% \end{table}


% \begin{figure*}[!htb]
% \centering
% \epsfig{file=rc248.eps, width=7in}
% \caption{Relative performace of reuse cache under varying data array sizes. The data array sizes for ratio 2, 4, 8 are 512KB, 256KB, 128KB, respectively.}
% \label{fig:rc248}
% \end{figure*}



% \begin{figure}[!htb]
% \centering
% \epsfig{file=bzip2live.eps, width=3.4in}
% \caption{bzip2 percentage of live lines during execution of workload. Sampling is once per 100K data accesses.}
% \label{fig:bzip2live}
% \end{figure}

% \begin{figure*}[!htb]
% \centering
% \epsfig{file=addarray.eps, width=7in}
% \caption{Relative performace of reuse cache under varying additional block array sizes. The additional block array sizes for ratio 16, 32, 64 are 64KB, 32KB, 16KB, respectively. No additional block array means a regular reuse cache design. All statistics are obtained when reuse cache tag/data ratio is 2.}
% \label{fig:addarray}
% \end{figure*}


% \begin{table*}[!htb]
% \centering
% \begin{tabular}{|c||c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{\textbf{Benchmark}} & 
% \multicolumn{2}{c|}{\textbf{Ratio 2}} & \multicolumn{2}{c}{\textbf{Ratio 4}} & \multicolumn{2}{|c|}{\textbf{Ratio 8}}\\
% \cline{2-7}
% & \textbf{Speedup} & \textbf{Miss Rate} & \textbf{Speedup} & \textbf{Miss Rate} & \textbf{Speedup} & \textbf{Miss Rate} \\ \hline
% astar & 0.983 & 1.046 & 0.984 & 1.042 & 0.972 & 1.069\\ \hline 
% bwaves & 0.923 & 1.064 & 0.935 & 1.079 & 0.945 & 1.067\\ \hline 
% bzip2 & 0.858 & 1.114 & 0.833 & 1.112 & 0.820 & 1.154\\ \hline 
% lbm & 0.966 & 1.010 & 0.974 & 0.981 & 0.973 & 0.982\\ \hline 
% libquantum & 0.957 & 1.015 & 0.964 & 1.020 & 0.979 & 1.045\\ \hline 
% mcf & 0.876 & 1.700 & 0.833 & 2.030 & 0.796 & 2.383\\ \hline 
% milc & 0.988 & 1.016 & 0.989 & 1.021 & 0.992 & 1.014\\ \hline 
% omnetpp & 0.973 & 1.689 & 0.940 & 3.202 & 0.848 & 8.054\\ \hline 
% \textbf{Average} & \textbf{0.939} &\textbf{1.070} &\textbf{0.930} & \textbf{1.091} &\textbf{0.913} &\textbf{1.148}\\ \hline
% \end{tabular}
% \caption{Speedup and miss rate of workloads relative to baseline. Ths miss rate is a relative ratio compared with baseline miss rate.}
% \label{table:missrate}
% \end{table*}

