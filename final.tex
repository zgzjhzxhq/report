\documentclass[letterpaper,11pt,twocolumn]{article}
\include{template}
\usepackage{multicol}
\usepackage{fullpage}
\usepackage{url}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{framed}
\usepackage{enumerate}
\pagenumbering{arabic}
\setlength{\columnsep}{0.25in}
\usepackage{paralist}
\let\itemize\compactitem

\def\bfw{\mathbf w}
\def\bfW{\mathbf W}
\def\bfD{\mathbf D}
\def\bfY{\mathbf Y}
\def\bfX{\mathbf X}
\def\bfU{\mathbf U}
\def\bfV{\mathbf V}
\def\bfS{\mathbf \Sigma}
\def\bfx{\mathbf x}
\def\R{\mathbb R}
\def\F{\mathrm F}

\title{\bf{Twitter hashtag implication using various machine learning methods}}
\author{
  Chang Wang\\
  Department of Computer Sciences\\
  \texttt{wych92@cs.wisc.edu}
  \and
  Lichao Yin\\
  Department of Computer Sciences\\
  \texttt{lichao@cs.wisc.edu}
  \and
  Chaowen Yu\\
  Department of Computer Sciences\\
  \texttt{ycw@cs.wisc.edu}
  \and
  Biao Zhang\\
  Department of Computer Sciences\\
  \texttt{bzhang263@wisc.edu}
}
\date{May 9, 2015}
\begin{document}
\twocolumn[
\maketitle
]
\begin{abstract}
\textbf{keywords}: supervised learning, text classification
\end{abstract}


\section{Introduction}
\label{sec:intro}

In this ever-changing world, information is produced and consumed ever faster than we could imagine. Every second, Twitter users post 6000 tweets, which corresponds to 350,000 tweets per minute, and 500 million tweets per day. With such amount of information generated, it can be formidable to search for a specific piece of information, if not completely impossible. That is why hashtag (\#) comes into being. Formally, hashtag (\#) is a character string that indicates the topic or category of the tweet. A tweet can have more than one hashtag and thus related to multiple topics. Hashtag greatly facilitates information filtering and rearrangements as searching for a tweet can be simplified to searching for a hashtag or even multiple hashtags.

However, not every user is used to hashtagging a tweet: they may forget to hashtag a tweet or they may use rare hashtags that could barely be keyword in searching or filtering. On the other hand, manually fixing a million tweets seems impractical even if tagless or bad-tagged tweets take up only 0.2\% of all tweets posted every day.

Our project aims to solve this problem using various machine learning methods. Given the text of a tweet, our task is to imply an appropriate hashtag that indicates the category or topic of this tweet. Formally, we define the hashtag implication problem as a supervised learning classification model as follows.

\begin{definition}[Hashtag Implication Problem]
Given a $S$ set of candidate hashtags as class labels, and a set $T$ of tagged tweets as training set. Learn a model $H$ that predicts one hashtag from $S$ for an untagged tweet with high accuracy.
\end{definition}

We do not specify deliberately which machine learning model we will be using as long as it is supervised learning. This is because it is not clear at this moment which model has the best performance. We feel it is wise to explore various machine learning methods, compare their performances and find out which machine learning method is the most accurate on hashtag implication problem.

The rest of our report is structured as follows: Section \ref{sec:data} summarizes how we obtain training and testing data using Twitter API. Section \ref{sec:impl} introduces implementation details for various machine learning methods including decision tree, neural network, support vector machine, etc. Performances of these methods are compared and analyzed in Section \ref{sec:resu}. Finally Section \ref{sec:conc} concludes this paper with some possible future work.

\section{Data Preparation}
\label{sec:data}

\subsection{Hashtag Selection}
Unlike many may conceive, topic selection is no easy task. To select a set of hashtag candidates, we consider several factors. First, a hashtag should have a unique textual form. If one topic can have multiple hashtag forms, it may not possible to classify a set of tweets because a tweet may be put into any hashtag class under the same topic. Effective classification is depending on disjoint hashtag classes thus disjoint topics. For example \texttt{\#butterfly} and \texttt{\#butterflies} have exactly the same meaning and may be used as hashtags interchangeably, and having a class for \texttt{\#butterfly} and another for \texttt{\#butterfiles} does not only undermine our classification, but also conflicts with our project initiatives.

Second, a hashtag should have enough tweets for training. Too few tweets associated with a hashtag cannot fully characterize the hashtag and may not achieve effective training process. We think 1000 tweets for a hashtag are sufficient for training and testing. In reality, multiple hashtags are also possible. In this scenario we simply treat other hashtags as regular words and leave the only hashtag selected in our candidate set.

Combining these factors, we selected five hashtags, each having 1000 associated tweets. They are
\begin{itemize}
\item
\texttt{\#badgers}
\item
\texttt{\#baltimoreuprising}
\item
\texttt{\#gameofthrones}
\item
\texttt{\#love}
\item
\texttt{\#taylorswift}
\end{itemize}

\subsection{Tweet Fetching}
To facilitate publich usage and research, Twitter has publicized various APIs via which interested users can retrieve information from Twitter web server \cite{twitterdocument}. There are two sets of Twitter APIs that we use. First, OAuth APIs \cite{twitteroauth} are used to send secured authorized requests. We need to provide authentication and authorization information whenever we want to use Twitter API. This includes applying for a new application called Hashtag Prefetcher on the user profile and get a pair of Consumer Key and Consumer Secret for application and a pair of Access Key and Access Secret for user account. The second part, once authorized, is to make queries to server using the search API \cite{twittersearch}. The search API is part of Twitter REST API that return tweets satisfying a certain set of conditions such as posted time, hashtag, keyword, language, retweet count, etc. It is written in JSON and returns a series of Tweet objects.

In our project, we use a simple Twitter API wrapper called tweepy \cite{tweepy}.

\subsection{Data Preprocessing}
After above steps, we get 5000 tweets with corresponding hashtags like this:
\begin{framed}
\texttt{
The House of Black and White - The Wars to Come - \#gameofthrones \textbf\#got \#got5 \#e2s5  \#hbo $\backslash$u2026 https://t.co/6zrQXilrZe
}
\end{framed}
It is obvious that \texttt{http://}, \texttt{$\backslash$u2026} and \texttt{\#gameofthrones} do little contribution to classification. So we find them out through 5000 tweets and remove them. For convenience, we then remove all non-alphanumeric characters and turn all letters to lowercase. Thus the example tweet turns to be:
\begin{framed}
\texttt{
the house of black and white the wars to come
}
\end{framed}
Because computer usually fails to capture semantic information from human languages, it is common to represent a text as a vector corresponding to the terms(usually terms are words) that appear in the text. For our problem, we will use bag-of-words model. Here we define the following terms as denoted as \cite{blei2003latent}:
\begin{itemize}
\item A \emph{term} is the basic unit of discrete data denoted by $w$, which is usually a word appearing in a tweet.
\item A \emph{tweet} contains $N$ terms, denoted by $\bfw = (w_1, w_2, \dots, w_N)$.
\item A \emph{corpus} is a collection of $M$ tweets($M=5000$). The tweets texts are denoted by $\bfD = \{\bfw_1, \bfw_2, \dots, \bfw_M\}$, and their corresponding tag labels are denoted by $\bfY = \{y_1, y_2, \dots, y_M\}$ where $y_i \in \{1, 2, 3, 4, 5\}, i = 1, 2, \dots, M$. For our problem, 1 stands for the hashtag \texttt{badgers}, 2 stands for the hashtag \texttt{baltimoreuprising}, 3 stands for the hashtag \texttt{gameofthrones}, 4 stands for the hashtag \texttt{love}, and 5 stands for the hashtag \texttt{taylorswift}.
\item A \emph{dictionary} is a collection of all $V$ unique terms appearing in the corpus denoted by $\bfW = \{w_1^*, w_2^*, \dots, w_V^*\}$.
\end{itemize}

Let $\bfX$ be a term-tweet matrix. This matrix has $M$ rows(one row for each tweets) and $V$ columns(one column for each unique term in dictionary). The element $x_{ij}$ in $\bfX$ indicates whether the $j$-th term appears in the $i$-th tweet. If it does appear, $x_{ij} = 1$; otherwise $x_{ij} = 0$.

In general, because most tweets will just use a small part of terms in the dictionary, a lot of $x_{ij}$ will be zero. So the matrix $\bfX$ is sparse. However, a sparse $\bfX$ wastes too much space and processes much slower in high dimension. As we know, there are some high-frequency terms appearing in all documents, such as function words(e.g., \emph{a}, \emph{or}, \emph{on}) and pronouns(e.g., \emph{which}, \emph{it}, \emph{those}). These terms have little contribution to representing different documents because of relatively low information content, so we shall remove them from our dictionary. \cite{salton1971smart} developed a SMART system with a popular list of more than 500 common terms, which called \emph{stop-word list} and can be used for our model.

After removing \emph{stop-words}, our final $\bfX$ has 5000 rows for tweets and 9791 columns for unique terms, and $\bfY$ has 5000 rows for each tweet's tag label.
\subsection{Cross Validation}
We will use stratified sampling for cross validation.

Since we have 1000 tweets for every 5 hashtags. We equally split each 1000 hashtag tweets into 5 parts. And each time we select and combine 4 parts from every hashtags for training and the rest part for testing.

Therefore, we have 5 datasets in all. Each dataset has training matricies: $\bfX_{training} \in \R^{4000 \times 9791}$ and $\bfY_{training} \in \{1, 2, 3, 4, 5\}^{4000}$, and testing matrices $\bfX_{testing} \in \R^{1000 \times 9791}$ and $\bfY_{testing} \in \{1, 2, 3, 4, 5\}^{1000}$.

\section{Implementation}
\label{sec:impl}
\subsection{Decision Tree}
Decision tree is a simple but powerful machine-learning model. The advantages of decision trees are: 1. Easy to interpret and explain. 2. Non-parametric, so you don't have to worry about outliers or whether the data is linearly separable. The disadvantage is also obvious that they easily overfit.

To learn a multi-class decision tree, we use the implementation from scikit-learn 1.6.1. scikit-learn uses an optimised version of the CART algorithm.

We use scikit-learn default configuration to finish our task. The default configurations are as follows:

\begin{itemize}
\item \emph{criterion}: Gini impurity function is used to measure the quality of a split.
\item \emph{splitter strategy}: choose the “best” at when splitting.
\item \emph{number of features to consider when looking for the best split}: consider max number of features at each split since all features are of integer type.
\item \emph{maximum depth of the tree}: nodes are expanded until all leaves are pure or until all leaves contain less than a threshold.
\item \emph{minimum number of samples for splitting}: The minimum number of samples required to split an internal node. Set to 2.
\item \emph{minimum number of samples for a leaf node }: The minimum number of samples required to be at a leaf node. Set to 1.

\end{itemize}
\subsection{Random Forest}
Random Forest is an ensemble method that produces a highly accurate classifier and learns fast. It runs efficiently on large data bases and handles large amount of input variables without variable deletion or selection.

We download an Matlab implementation code from \emph{https://code.google.com/p/randomforest-matlab}. For our problem we set the number of trees is 500 and the maximum trial times is 98.

\subsection{Neural Network}
Neural networks are algorithms that can be used to perform nonlinear statistical modeling and provide a new alternative to logistic regression, the most commonly used method for developing predictive models. Neural network requires less formal statistical training and implicitly detects complex nonlinear relationships between dependent and independent variables.

For our classification problem, we use neural network toolbox in matlab. We just use one hidden layer with 10 neurons and the other configurations are as follows:
\begin{itemize}
\item \emph{Data Division}: Random (dividerand)
\item \emph{Training}: Scaled Conjugate Gradient (trainscg)
\item \emph{Performance}: Cross-Entropy (crossentropy)
\item \emph{Derivative}: Default (defaultderiv)
\end{itemize}

\subsection{Na{\"i}ve Bayes}
The na{\"i}ve Bayes algorithm is one of the two classic na{\"i}ve Bayes variants used in text classification where the data are typically represented as word vector counts. The biggest advantages of na{\"i}ve Bayes is that it is super simple. Furthermore, sometimes even if the na{\"i}ve Bayes assumption doesn't hold, a na{\"i}ve Bayes classifier still often performs surprisingly well in practice.

The implementation of na{\"i}ve Bayes we used is the multinomial na{\"i}ve Bayes MultinomialNB from scikit-learn 1.6.1.

The configuration we used in our project is the default one, which are as follows:
\begin{itemize}
\item \emph{additive (Laplace/Lidstone) smoothing parameter }: 1.0
\item \emph{whether to learn class prior probabilities or not}: True
\item \emph{prior probabilities of the classes}: None
\end{itemize}

\subsection{Support Vector Machine}
Advantages of SVMs: High accuracy, nice theoretical guarantees regarding overfitting, and with an appropriate kernel they can work well even if you're data isn't linearly separable in the base feature space. Especially popular in text classification problems where very high-dimensional spaces are the norm. Memory-intensive and kind of annoying to run and tune, though, so I think random forests are starting to steal the crown.

SVM uses kernel trick to map inputs to higher dimension so that they can be linear classified.

Parameters setting:
\begin{itemize} 
\item \emph{kernel function:} Gaussian function
\item \emph{sigma:} number of features
\end{itemize}

\subsection{Logistic Regression Model}
Logistic Regression can deal with binomial or multinomial classification problems. It can be seen a simplified version of sigmoid neural network without hidden layer. The result can be seen the confidence measure of every category.

Here we used LIBLINEAR to train and test the tweet data.

\subsection{$k$-Nearest Neighbors}
The k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method. So we don't have to tune the model parameters. As an instanced-based algorithm where every prediction is dependent on local information, it is very sensitive to local structure.

We used Matlab built-in kNN implementation \texttt{fitcknn} with $k = 1$ an $k = 5$.

\section{Results}
\label{sec:resu}
In detail, we find something interesting to address. Even if na{\"i}ve Bayes is not the implementing algorithm with best performance, it indeed draws some outstanding predictions in some cases, which shows its availability and accuracy. For example, all other implementing algorithms predicts the following tweet as \texttt{love} and only na{\"i}ve Bayes predicts it as \texttt{badgers}.
\begin{framed}
\texttt{
I may have cried when illustrating @AngelaSlatter's "Spells for Coming Forth By Daylight" http://t.co/9IMhTRLj49 #books #badgers #endings
}
\end{framed}
Another thing that shall be addressed is that our dataset does have some noise. In our first dataset, we find that all implementing algorithms predict wrong for the 528th testing tweet. It shall belongs to the hashtag \texttt{gameofthrones}, but several algorithms treat it as \texttt{love}, \texttt{badgers} or \texttt{baltimoreuprising}. That's because the original content of the tweet is too simple and "common", which is
\begin{framed}
\texttt{
Yep... we've all been there. $\backslash$ud83d$\backslash$ude29 http://t.co/XTucN2D4yx  \#ILoveMondays \#Sarcasm \#GameofThrones \#GoT \#Joffrey
}
\end{framed}
It is obvious that this tweet contains no outstanding semantic terms which indicates \texttt{gameofthrones} meaning. And this case is not unique in our dataset, which affects the overall performance.

\begin{figure*}[!htb]
\centering
\epsfig{file=acc.eps, scale=0.45}
\caption{Boot-up time for various configurations}
\label{figure:eval}
\end{figure*}

\begin{figure*}[!htb]
\centering
\epsfig{file=label.eps, scale=0.38}
\caption{Boot-up time for various configurations}
\label{figure:eval}
\end{figure*}

% \begin{figure*}[!htb]
% \centering
% \epsfig{file=latencies.eps, width=6.8in}
% \caption{Latencies for various IPC methods on MicroswiftOS and archlinux}
% \label{figure:latency}
% \end{figure*}

% \begin{figure*}[!htb]
% \centering
% \epsfig{file=throughput.eps, width=6.8in}
% \caption{Throughputs for various IPC methods on MicroswiftOS and archlinux}
% \label{figure:throughput}
% \end{figure*}

\section{Conclusion}
\label{sec:conc}

% \bibliographystyle{abbrv}
\bibliographystyle{plain}
\bibliography{references}

\end{document}

% \begin{table}[!htb]
% \centering
% \begin{tabular}{|c||c|c|c|}
% \hline
% \multirow{3}{*}{\textbf{Benchmark}} & \textbf{Additional} & \textbf{Total} &\multirow{3}{*}{\textbf{Percentage}}\\ 
% &\textbf{Memory} &\textbf{Memory}&\\
% &\textbf{Accesses} &\textbf{Accesses} &\\ \hline \hline
% aster & 175481 & 2404970 & 0.0730\\ \hline 
% bzip2 & 1067380 & 9188892 & 0.1162\\ \hline 
% lbm   & 130990 & 5003775 & 0.0262\\ \hline 
% libquantum   & 54796 & 3092800 & 0.0177\\ \hline 
% milc  & 136746 & 5575859 & 0.0245\\ \hline 
% omnetpp  & 107165 & 287076 & 0.3733\\ \hline 
% \textbf{Average} & \textbf{-} &\textbf{- } &\textbf{0.1051}\\ \hline
% \end{tabular}
% \caption{Additional memory accesses for storing reuse data in a 1MB L2 reuse cache during a simulation period of 300M instructions}
% \label{table:additionalaccess}
% \end{table}

% \begin{figure}[!htb]
% \centering
% \epsfig{file=ReuseStruct.eps, height =2in, width =2in}
% \caption{Structure of our implementation for the reuse cache}
% \label{reuse:struct}
% \end{figure}

% \begin{table}[!htb]
% \centering
% \begin{tabular}{|c||c|c|c|}
% \hline
% \multirow{3}{*}{\textbf{benchmark}} & \textbf{memory} & \textbf{total} & \multirow{3}{*}{\textbf{percentage}}\\
% &\textbf{write} &\textbf{memory} &\\
% & \textbf{miss times} & \textbf{access times} & \\ \hline\hline
% bzip2 & 778893 & 16791412 & 0.0464\\ \hline 
% aster & 83555 & 4487131 & 0.0186\\ \hline 
% libquantum & 35243 & 6091860 & 0.0058\\ \hline 
% lbm & 72942 & 9862259 & 0.0074\\ \hline 
% milc & 69407 & 10820712 & 0.0064\\ \hline 
% omnetpp & 83508 & 416317 & 0.2006 \\ \hline

% \textbf{Average} &\textbf{-} &\textbf{-} &\textbf{0.0475}\\

% \hline
% \end{tabular}
% \caption{Memory write miss times in a 1MB reuse cache during a simulation period of 300M instructions}
% \label{table:writemiss}
% \end{table}



% \begin{table}[!htb]
% \centering
% \begin{tabular}{|l||r|}
% \hline
% L1 data cache size & 64KB\\
% \hline
% L1 data cache associativity & 4-way\\
% \hline
% L1 instruction cache size & 32KB\\
% \hline
% L1 instruction cache associativity & 4-way\\
% \hline
% L2 cache size & 1MB\\
% \hline
% L2 cache associativity & 8-way\\
% \hline
% Cache line size & 64B\\
% \hline
% Single memory size & 512 MB\\
% \hline
% \end{tabular}
% \caption{System configurations for baseline performance}
% \label{table:baselineconfig}
% \end{table}


% \begin{figure*}[!htb]
% \centering
% \epsfig{file=rc248.eps, width=7in}
% \caption{Relative performace of reuse cache under varying data array sizes. The data array sizes for ratio 2, 4, 8 are 512KB, 256KB, 128KB, respectively.}
% \label{fig:rc248}
% \end{figure*}



% \begin{figure}[!htb]
% \centering
% \epsfig{file=bzip2live.eps, width=3.4in}
% \caption{bzip2 percentage of live lines during execution of workload. Sampling is once per 100K data accesses.}
% \label{fig:bzip2live}
% \end{figure}

% \begin{figure*}[!htb]
% \centering
% \epsfig{file=addarray.eps, width=7in}
% \caption{Relative performace of reuse cache under varying additional block array sizes. The additional block array sizes for ratio 16, 32, 64 are 64KB, 32KB, 16KB, respectively. No additional block array means a regular reuse cache design. All statistics are obtained when reuse cache tag/data ratio is 2.}
% \label{fig:addarray}
% \end{figure*}


% \begin{table*}[!htb]
% \centering
% \begin{tabular}{|c||c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{\textbf{Benchmark}} & 
% \multicolumn{2}{c|}{\textbf{Ratio 2}} & \multicolumn{2}{c}{\textbf{Ratio 4}} & \multicolumn{2}{|c|}{\textbf{Ratio 8}}\\
% \cline{2-7}
% & \textbf{Speedup} & \textbf{Miss Rate} & \textbf{Speedup} & \textbf{Miss Rate} & \textbf{Speedup} & \textbf{Miss Rate} \\ \hline
% astar & 0.983 & 1.046 & 0.984 & 1.042 & 0.972 & 1.069\\ \hline 
% bwaves & 0.923 & 1.064 & 0.935 & 1.079 & 0.945 & 1.067\\ \hline 
% bzip2 & 0.858 & 1.114 & 0.833 & 1.112 & 0.820 & 1.154\\ \hline 
% lbm & 0.966 & 1.010 & 0.974 & 0.981 & 0.973 & 0.982\\ \hline 
% libquantum & 0.957 & 1.015 & 0.964 & 1.020 & 0.979 & 1.045\\ \hline 
% mcf & 0.876 & 1.700 & 0.833 & 2.030 & 0.796 & 2.383\\ \hline 
% milc & 0.988 & 1.016 & 0.989 & 1.021 & 0.992 & 1.014\\ \hline 
% omnetpp & 0.973 & 1.689 & 0.940 & 3.202 & 0.848 & 8.054\\ \hline 
% \textbf{Average} & \textbf{0.939} &\textbf{1.070} &\textbf{0.930} & \textbf{1.091} &\textbf{0.913} &\textbf{1.148}\\ \hline
% \end{tabular}
% \caption{Speedup and miss rate of workloads relative to baseline. Ths miss rate is a relative ratio compared with baseline miss rate.}
% \label{table:missrate}
% \end{table*}

