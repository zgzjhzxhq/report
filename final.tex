\documentclass[letterpaper,11pt,twocolumn]{article}
\include{template}
\usepackage{multicol}
\usepackage{fullpage}
\usepackage{url}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{framed}
\usepackage{enumerate}
\pagenumbering{arabic}
\setlength{\columnsep}{0.25in}
\usepackage{paralist}
\let\itemize\compactitem

\def\bfw{\mathbf w}
\def\bfW{\mathbf W}
\def\bfD{\mathbf D}
\def\bfY{\mathbf Y}
\def\bfX{\mathbf X}
\def\bfU{\mathbf U}
\def\bfV{\mathbf V}
\def\bfS{\mathbf \Sigma}
\def\bfx{\mathbf x}
\def\R{\mathbb R}
\def\F{\mathrm F}

\title{\bf{Twitter hashtag implication using various machine learning methods}}
\author{
  Chang Wang\\
  Department of Computer Sciences\\
  \texttt{wych92@cs.wisc.edu}
  \and
  Lichao Yin\\
  Department of Computer Sciences\\
  \texttt{lyin28@wisc.edu}
  \and
  Chaowen Yu\\
  Department of Computer Sciences\\
  \texttt{ycw@cs.wisc.edu}
  \and
  Biao Zhang\\
  Department of Computer Sciences\\
  \texttt{bzhang263@wisc.edu}
}
\date{May 9, 2015}
\begin{document}
\twocolumn[
\maketitle
]
\begin{abstract}
\textbf{keywords}: text classification
\end{abstract}


\section{Introduction}
\label{sec:intro}
In this ever-changing world, information is produced and consumed ever faster than we could imagine. Every second, Twitter users post 6000 tweets, which corresponds to 350,000 tweets per minute, and 500 million tweets per day. With such amount of information generated, it’s fairly hard to synthesize and search a specific piece of information. That is why hashtag (#) comes to facilitate information filtering and rearrangements. However, users may forget to hashtag a tweet or even worse, they may use inappropriate hashtags that could barely be keyword in searching and filtering tweets. On the other hand, manually fixing a million tweets seems impractical even if tagless or bad-tagged tweets take up only 0.2% of all tweets posted every day. 
We would like to solve this problem – hashtag implication – using a probabilistic machine learning models. Even though we have not yet come up with specific models we will be using, we feel it is worthwhile to explore this open problem. We split our project into four phases, each focusing on a specific problem built on top of previous phase.
Phase one: single hashtag implication for an offline set of tweets and predefined hashtag set. This means we first need to collect an amount of hashtagged tweets from Twitter and this constitutes the offline set. (The meaning of offline will be clear in following phases.) Their hashtags constitutes the fixed hashtag set. We can learn from hashtagged tweets and find specific patterns described by model parameters. Then by using this probabilistic machine learning model, we imply a hashtag from the fixed tag set for a tweet without hashtag.
Phase two: multiple hashtag implication for an offline set of tweets and predefined hashtag set. In this phase, we extend single hashtag to multiple hashtags. And this brings new challenges including how many tags should a tweet have? Can tweets have different number of hashtags and what is a proper threshold for this? Can we leave some tweets untagged because no hashtag is appropriate in the fixed hashtag set?
Phase three: multiple hashtag implication for an offline set of tweets with learnt hashtag set. We focus on learning a plausible hashtag set from tweets collected. And then apply algorithm in phase two to imply hashtags for new tweets. This learnt hashtag set should be 1) appropriate in size, meaning neither containing too many hashtags nor too few, and 2) uniform in topics. We elaborate uniformity here to clarify. We believe that hashtags covering same topics are similar thus too many hashtags in the same topic can be redundant. Such redundancy will 1) tremendously increase keyword search time and 2) induce hashtag hazard, that is, the algorithm in phase two will find multiple hashtags equally suitable for a tweet and does not know which subset of them are the most suitable because all hashtags implied are essentially the same. A good example is a tweet with three hashtags #brazil2014soccer #brazilsoccer2014 and #2014worldcup, users may get bored finding out hashtags are even longer than the tweet text itself.
Phase four: multiple hashtag implication for an online set of tweets with growing hashtag set. In this setting, everything is exactly the same as phase three except that each tweet comes online. This is an exact imitation of real world tweets flow. Initially there is not posted tweet and no hashtag in the set. Then tweets are coming one by one, and we need to decide weather or not add a new hashtag into our set due to more tweets on a specific topic, or eliminate a hashtag because the fraction of tweets on that topic drops below some threshold or hashtag set is too big. A simplest implementation would be running phase three algorithm upon each tweet arrival. But this could be inefficient and impractical in the growing amount of work of regression. An easy optimization would be rerunning phase three algorithm once every 100 new tweets. We can argue a good tradeoff between accuracy and efficiency. We can also explore other optimizations too.


\section{Data Preparation}
\label{sec:data}
\subsection{Data Collection}
\subsection{Data Preprocessing}
After above steps, we get 5000 tweets with corresponding hashtags like this:
\begin{framed}
The House of Black and White - The Wars to Come - \#gameofthrones \#got \#got5 \#e2s5  \#hbo $\backslash$u2026 https://t.co/6zrQXilrZe
\end{framed}
It is obvious that \emph{http://}, \emph{$\backslash$u2026} and \emph{\#gameofthrones} do little contribution to classification. So we find them out through 5000 tweets and remove them. For convinience, we then remove all non-alphanumeric characters and turn all letters to lowercase. Thus the example tweet turns to be:
\begin{framed}
the house of black and white the wars to come
\end{framed}
Because computer usually fails to capture semantic information from human languages, it is common to represent a text as a vector corresponding to the terms(usually terms are words) that appear in the text. For our problem, we will use bag-of-words model. Here we define the following terms as denoted as \cite{blei2003latent}:
\begin{itemize}
\item A \emph{term} is the basic unit of discrete data denoted by $w$, which is usually a word appearing in a tweet.
\item A \emph{tweet} contains $N$ terms, denoted by $\bfw = (w_1, w_2, \dots, w_N)$.
\item A \emph{corpus} is a collection of $M$ tweets($M=5000$). The tweets texts are denoted by $\bfD = \{\bfw_1, \bfw_2, \dots, \bfw_M\}$, and their corresponding tag labels are denoted by $\bfY = \{y_1, y_2, \dots, y_M\}$ where $y_i \in \{1, 2, 3, 4, 5\}, i = 1, 2, \dots, M$. For our problem, 1 stands for the hashtag "", 2 stands for the hashtag "", 3 stands for the hashtag "", 4 stands for the hashtag "", and 5 stands for the hashtag "".
\item A \emph{dictionary} is a collection of all $V$ unique terms appearing in the corpus denoted by $\bfW = \{w_1^*, w_2^*, \dots, w_V^*\}$.
\end{itemize}
Let $\bfX$ be a term-tweet matrix. This matrix has $M$ rows(one row for each tweets) and $V$ columns(one column for each unique term in dictionary). The element $x_{ij}$ in $\bfX$ indicates whether the $j$-th term appears in the $i$-th tweet. If it does appear, $x_{ij} = 1$; otherwise $x_{ij} = 0$.\\
In general, because most tweets will just use a small part of terms in the dictionary, a lot of $x_{ij}$ will be zero. So the matrix $\bfX$ is sparse. However, a sparse $\bfX$ wastes too much space and processes much slower in high dimension. As we know, there are some high-frequency terms appearing in all documents, such as function words(e.g., \emph{a}, \emph{or}, \emph{on}) and pronouns(e.g., \emph{which}, \emph{it}, \emph{those}). These terms have little contribution to representing different documents because of relatively low information content, so we shall remove them from our dictionary. \cite{salton1971smart} developed a SMART system with a popular list of more than 500 common terms, which called \emph{stop-word list} and can be used for our model.\\
After removing \emph{stop-words}, our final $\bfX$ has 5000 rows for tweets and 9791 columns for unique terms, and $\bfY$ has 5000 rows for each tweet's tag label.
\subsection{Cross Validation}
We will use stratified sampling for cross validation.\\
Since we have 1000 tweets for every 5 hashtags. We equally split each 1000 hashtag tweets into 5 parts. And each time we select and combine 4 parts from every hashtags for training and the rest part for testing.\\
Therefore, we have 5 datasets in all. Each dataset has training matricies: $\bfX_{training} \in \R^{4000 \times 9791}$ and $\bfY_{training} \in \{1, 2, 3, 4, 5\}^{4000}$, and testing matrices $\bfX_{testing} \in \R^{1000 \times 9791}$ and $\bfY_{testing} \in \{1, 2, 3, 4, 5\}^{1000}$.

\section{Implementation}
\label{sec:impl}
\subsection{Decision Tree}
To lean a multi-class decision tree, we use the implementation from scikit-learn 1.6.1. scikit-learn uses an optimised version of the CART algorithm.\\
\subsection{Random Forest}

\subsection{Neural Network}
\subsection{Naive Bayes}
\subsection{Support Vector Machine}
\subsection{Logistic Regression Model}
\subsection{$k$ Nearest Neighbors}


\section{Results}
\label{sec:eval}
% \begin{figure}[!htb]
% \centering
% \epsfig{file=evaluation.eps, width=3.4in}
% \caption{Boot-up time for various configurations}
% \label{figure:eval}
% \end{figure}

% \begin{figure*}[!htb]
% \centering
% \epsfig{file=latencies.eps, width=6.8in}
% \caption{Latencies for various IPC methods on MicroswiftOS and archlinux}
% \label{figure:latency}
% \end{figure*}

% \begin{figure*}[!htb]
% \centering
% \epsfig{file=throughput.eps, width=6.8in}
% \caption{Throughputs for various IPC methods on MicroswiftOS and archlinux}
% \label{figure:throughput}
% \end{figure*}

\section{Related Works}
\label{sec:related}
\section{Future Work}
\label{sec:future}

\section{Acknowledgement}
We would like to thank Michael Swift, Sanketh Nalli and Zhaoyu Luo for their helpful advices and comments. 



% \bibliographystyle{abbrv}
\bibliographystyle{plain}
\bibliography{references}

\end{document}

% \begin{table}[!htb]
% \centering
% \begin{tabular}{|c||c|c|c|}
% \hline
% \multirow{3}{*}{\textbf{Benchmark}} & \textbf{Additional} & \textbf{Total} &\multirow{3}{*}{\textbf{Percentage}}\\ 
% &\textbf{Memory} &\textbf{Memory}&\\
% &\textbf{Accesses} &\textbf{Accesses} &\\ \hline \hline
% aster & 175481 & 2404970 & 0.0730\\ \hline 
% bzip2 & 1067380 & 9188892 & 0.1162\\ \hline 
% lbm   & 130990 & 5003775 & 0.0262\\ \hline 
% libquantum   & 54796 & 3092800 & 0.0177\\ \hline 
% milc  & 136746 & 5575859 & 0.0245\\ \hline 
% omnetpp  & 107165 & 287076 & 0.3733\\ \hline 
% \textbf{Average} & \textbf{-} &\textbf{- } &\textbf{0.1051}\\ \hline
% \end{tabular}
% \caption{Additional memory accesses for storing reuse data in a 1MB L2 reuse cache during a simulation period of 300M instructions}
% \label{table:additionalaccess}
% \end{table}

% \begin{figure}[!htb]
% \centering
% \epsfig{file=ReuseStruct.eps, height =2in, width =2in}
% \caption{Structure of our implementation for the reuse cache}
% \label{reuse:struct}
% \end{figure}

% \begin{table}[!htb]
% \centering
% \begin{tabular}{|c||c|c|c|}
% \hline
% \multirow{3}{*}{\textbf{benchmark}} & \textbf{memory} & \textbf{total} & \multirow{3}{*}{\textbf{percentage}}\\
% &\textbf{write} &\textbf{memory} &\\
% & \textbf{miss times} & \textbf{access times} & \\ \hline\hline
% bzip2 & 778893 & 16791412 & 0.0464\\ \hline 
% aster & 83555 & 4487131 & 0.0186\\ \hline 
% libquantum & 35243 & 6091860 & 0.0058\\ \hline 
% lbm & 72942 & 9862259 & 0.0074\\ \hline 
% milc & 69407 & 10820712 & 0.0064\\ \hline 
% omnetpp & 83508 & 416317 & 0.2006 \\ \hline

% \textbf{Average} &\textbf{-} &\textbf{-} &\textbf{0.0475}\\

% \hline
% \end{tabular}
% \caption{Memory write miss times in a 1MB reuse cache during a simulation period of 300M instructions}
% \label{table:writemiss}
% \end{table}



% \begin{table}[!htb]
% \centering
% \begin{tabular}{|l||r|}
% \hline
% L1 data cache size & 64KB\\
% \hline
% L1 data cache associativity & 4-way\\
% \hline
% L1 instruction cache size & 32KB\\
% \hline
% L1 instruction cache associativity & 4-way\\
% \hline
% L2 cache size & 1MB\\
% \hline
% L2 cache associativity & 8-way\\
% \hline
% Cache line size & 64B\\
% \hline
% Single memory size & 512 MB\\
% \hline
% \end{tabular}
% \caption{System configurations for baseline performance}
% \label{table:baselineconfig}
% \end{table}


% \begin{figure*}[!htb]
% \centering
% \epsfig{file=rc248.eps, width=7in}
% \caption{Relative performace of reuse cache under varying data array sizes. The data array sizes for ratio 2, 4, 8 are 512KB, 256KB, 128KB, respectively.}
% \label{fig:rc248}
% \end{figure*}



% \begin{figure}[!htb]
% \centering
% \epsfig{file=bzip2live.eps, width=3.4in}
% \caption{bzip2 percentage of live lines during execution of workload. Sampling is once per 100K data accesses.}
% \label{fig:bzip2live}
% \end{figure}

% \begin{figure*}[!htb]
% \centering
% \epsfig{file=addarray.eps, width=7in}
% \caption{Relative performace of reuse cache under varying additional block array sizes. The additional block array sizes for ratio 16, 32, 64 are 64KB, 32KB, 16KB, respectively. No additional block array means a regular reuse cache design. All statistics are obtained when reuse cache tag/data ratio is 2.}
% \label{fig:addarray}
% \end{figure*}


% \begin{table*}[!htb]
% \centering
% \begin{tabular}{|c||c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{\textbf{Benchmark}} & 
% \multicolumn{2}{c|}{\textbf{Ratio 2}} & \multicolumn{2}{c}{\textbf{Ratio 4}} & \multicolumn{2}{|c|}{\textbf{Ratio 8}}\\
% \cline{2-7}
% & \textbf{Speedup} & \textbf{Miss Rate} & \textbf{Speedup} & \textbf{Miss Rate} & \textbf{Speedup} & \textbf{Miss Rate} \\ \hline
% astar & 0.983 & 1.046 & 0.984 & 1.042 & 0.972 & 1.069\\ \hline 
% bwaves & 0.923 & 1.064 & 0.935 & 1.079 & 0.945 & 1.067\\ \hline 
% bzip2 & 0.858 & 1.114 & 0.833 & 1.112 & 0.820 & 1.154\\ \hline 
% lbm & 0.966 & 1.010 & 0.974 & 0.981 & 0.973 & 0.982\\ \hline 
% libquantum & 0.957 & 1.015 & 0.964 & 1.020 & 0.979 & 1.045\\ \hline 
% mcf & 0.876 & 1.700 & 0.833 & 2.030 & 0.796 & 2.383\\ \hline 
% milc & 0.988 & 1.016 & 0.989 & 1.021 & 0.992 & 1.014\\ \hline 
% omnetpp & 0.973 & 1.689 & 0.940 & 3.202 & 0.848 & 8.054\\ \hline 
% \textbf{Average} & \textbf{0.939} &\textbf{1.070} &\textbf{0.930} & \textbf{1.091} &\textbf{0.913} &\textbf{1.148}\\ \hline
% \end{tabular}
% \caption{Speedup and miss rate of workloads relative to baseline. Ths miss rate is a relative ratio compared with baseline miss rate.}
% \label{table:missrate}
% \end{table*}

